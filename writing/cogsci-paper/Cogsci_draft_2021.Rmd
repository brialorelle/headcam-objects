---
title: "Characterizing the object categories two children see and interact within a dense dataset of naturalistic visual experience"
bibliography: references.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\bf Bria Long}$^{1}$, {\bf George Kachergis}$^{1}$, {\bf Naiti Bhatt}$^{2}$, {\bf Michael C. Frank}$^{1}$ \\ 
         bria@stanford.edu, kachergis@stanford.edu, bhatt@g.hmc.edu, mcfrank@stanford.edu \\
         $^{1}$ Department of Psychology, Stanford University \\
         $^{2}$ Scripps College
         }

abstract: >
    What do infants and young children tend to see in their everyday lives?  Relatively little work has examined the categories and objects that tend to be in the infant view during everyday experience, despite the fact that this knowledge is central to theories of category learning. Here, we analyzed the prevalence of the  categories (e.g., people, animals, food) in the infant view in a longitudinal dataset of egocentric infant visual experience. Overall, we found a surprising amount of consistency in the broad characteristics of children's visual environment across individuals and across developmental time, in contrast to prior work examining the changing nature of the social signals in the infant view. In addition, we analyzed the distribution and identity of the categories that children tended touch and interact with in this dataset, generalizing previous findings that these objects tended to be distributed in a Zipfian manner. Taken together, these findings take a first step towards characterizing infants' changing visual environment, and call for future work to examine the generalizability of these results and to link them to learning outcomes.


keywords: >
    Object categorization; infant visual experience; head-mounted cameras; longitudinal data.

output: cogsci2016::cogsci_paper
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F,
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=T,
                      message=F, sanitize = T)

```

# Introduction
What do children tend to see in their everyday lives?
While an understanding of children's visual environment is central to both theories of language acquisition and visual development, we know remarkably little about the categories and objects that tend to be in the infant view, or in what format they are experienced.
For example, how often do infants tend to see animals in real-life vs. in storybooks or as toys?
How consistent are children's visual environments across individuals and across developmental time?

Over the past decade, researchers have begun to answer these questions by documenting the infant egocentric perspective using head-mounted cameras [@yoshida2008; @franchak2011] and quantifying the degree to which there are substantial shifts in infants' viewpoints that may have downstream developmental consequences. As adults, it is hard to intuit how strange this viewpoint can be, and how much it varies across development, transitioning over the first two years of life from close-up views of faces to restricted views of hands manipulating objects [@fausey2016; @long2020], with children's postural developments to a large extent shaping what they see [@sanchez2018postural].
Most work, however, has focused on documenting the social information that infants and children have access to across early development [@yoshida2008; @sanchez2018postural; @fausey2016]. 

```{r examples, include = T, fig.env = "figure*", fig.pos = "h", fig.align='center', out.width = '\\textwidth', set.cap.width=T, num.cols.cap=2, fig.align = "center", fig.cap = "Example frames with annotations of four different categories."}
examples <- png::readPNG("figs/montage.001.png")
grid::grid.raster(examples)
```

More recent research has made progress towards understanding what objects tend to be the infant view, starting with analyzing the basic-level categories (e.g., spoons, cups) in the view of 8-month-olds during mealtime. This work suggests that a small number of objects are both pervasively present during mealtime and among infants' first-learned words [@clerkin2017], pointing towards a link between visual experience and early word learning and category learning. 

Thus, a more complete understanding of the visual environment of infants and young children could yield insights about the inputs to both category learning and word learning. Indeed, different distributions of these visual referents lead to constraints on the kinds of learning mechanisms that must operate to form robust category representations -- and to learn words for these categories. However, at present, no datasets are sufficiently annotated to constrain these theoretical accounts.  

For example, if the categories in the infant view shift dramatically over the first few years of life, then we might expect infants to learn about certain categories earlier vs. later during development. Prior work documenting the proportion of social information in view has suggested that children see more hands relative to faces in this same age range [@fausey2016; @long2020]. Thus, one possibility is that as children learn to crawl and walk [@sanchez2018postural; @long2020; @franchak2011], categories that children are likely to interact with (i.e., toys, small objects) may also become more prevalent in the child's view. If this was the case, this finding would support a view where the inputs to early category learning are shaped by children's own ability to actively explore their environment.

On the other hand, the broad characteristics of children's visual environments may be relatively stable and mostly determined by the activities that they tend to engage in. Indeed, some theoretical accounts have suggested that the statistics of children's visual environment are mostly driven by these stereotyped activity contexts [@bruner1985role] -- e.g. mealtime or storytime -- and that children learn most robustly in these contexts. On these accounts, children might become very sensitive to the co-occurrences between different activities (e.g., eating) and object categories (e.g., spoons, food). However, no work has identified how consistently categories co-occur in natural environments. For example, while some activity contexts  (e.g., storytime)  lead to intuitive co-occurrences between object categories (e.g., between books and people), not all activity contexts will generate intuitive or consistent co-occurrences between object categories.

Finally, _how_ infants interact with object categories will undoubtedly change what they learn about them. For example, children tend to generate informative views of objects while manipulating them -- and, early in development, children's ability to sit and manipulate objects correlates with their perceptual abilities [@soska2010systems]. Yet while most datasets used to train deep neural network models contain only photographs of object categories, many children -- especially those in Western, industrialized cultures -- will likely experience many categories through picture books as flat, stylized, 2D depictions. If children see very few real-life exemplars of a category relative to depictions (e.g., giraffes), this suggests that children must learn to generalize between these different visual formats in order to group these exemplars into one category. Further, this implies that these representations might be coarser than those experienced across many different formats. And if children only interact and manipulate a few small set of categories -- as suggested by Clerkin et al., 2017 -- children may first learn about these frequently experienced categories and then use these representations to generalize to the categories they encounter very infrequently.  

```{r}
num_frames = 44000000 # in total dataset
fps=30 # frame rate
num_seconds = num_frames/fps
num_minutes = num_seconds/60
num_hours = num_minutes/60
num_days = num_hours/24 
sample = 24000
rate_sampled = round(sample / num_hours,0)
```

Here, we take a step towards answering these questions by characterizing the visual environment of two young children in a longitudinal corpus of head-mounted camera data [@SAYcam] from 6-32 months of age. To characterize trends in the visual environment over development, we collected annotations of several categories of objects (e.g., *animals*, *vehicles*, *toys*, *food*, *furniture*) present in the infant view, obtaining annotations on a randomly sampled set of 24,000 frames (i.e. around `r rate_sampled` frames per hour of recorded video). To provide a closer look into the kinds of objects children have the most intensive visual and haptic experience with, we also examined the specific objects that children interacted with during everyday activities. To do so, we annotated the basic-level identities (e.g., *spoon*, *marker*) of the objects that children were interacting with in the subset of frames where children's hands were visible.


```{r setup, echo=FALSE}
library(tidyverse)
library(here)
library(ggthemes)
library(viridis)
library(egg)
library(igraph)

```

```{r load-data, echo=FALSE}
load(file = here::here('data/preprocessed_data/merged_annotations.RData'))
```

```{r load-reliability} 
## load reliability outputs for both
load(file=here::here('data/preprocessed_data/reliabilty_stats.RData'))

sanity = read_csv(file = here::here('data/annotations/broad_categories/spotcheck_feb1.csv')) 
sanity_check <- sanity %>%
  as_tibble() %>%
  group_by(reason) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

total_images = length(sanity$img)
```

```{r freq-by-category-data, echo=FALSE}
freq_by_category <- d %>%
  group_by(category) %>%
  dplyr::summarize(count = n()) %>%
  mutate(freq_category = count / sum(count)) %>%
  arrange(desc(freq_category)) %>%
  mutate(category = fct_reorder(category, freq_category, .desc=TRUE))

freq_by_categories_by_age <- d %>%
  group_by(age_day_bin, child_id) %>%
  mutate(count_frames = n())  %>%
  ungroup() %>%
  group_by(category, age_day_bin, child_id) %>%
  dplyr::summarize(count_categories = n(), count_frames = count_frames[1]) %>%
  mutate(freq = count_categories / count_frames) %>%
  left_join(freq_by_category) %>%
  ungroup() %>%
  mutate(category = forcats::fct_reorder(category, freq_category, .desc=TRUE)) %>%
  mutate(data_type = "All frames")
```

```{r child-hands}
d_hands <- d %>%
  filter(child_hand_seg == TRUE)

freq_by_categories_by_age_child_hands <- d_hands %>%
  group_by(age_day_bin, child_id) %>%
  mutate(count_frames = n())  %>%
  ungroup() %>%
  group_by(category, age_day_bin, child_id) %>%
  dplyr::summarize(count_categories = n(), count_frames = count_frames[1]) %>%
  mutate(freq = count_categories / count_frames) %>%
  left_join(freq_by_category) %>%
  ungroup() %>%
  mutate(category = forcats::fct_reorder(category, freq_category, .desc=TRUE))  %>%
  mutate(data_type = "Childs hands in view") %>%
  full_join(freq_by_categories_by_age)
```


```{r freq_by_category, echo=F, include = T, fig.env = "figure*", fig.pos = "h",num.cols.cap=2, out.width = '\\textwidth', fig.width=8, fig.align = "center", fig.cap = "Frequency of categories annotated across the 24K random frames plotted as a function of each child's age (in months); each child's age was calculated in days relative to the date that the videos were filmed and converted to months. Each color represents data from a different child."}
## Figure 2

ggplot(data=freq_by_categories_by_age, aes(x=age_day_bin, y=(freq), size=count_frames, col=child_id)) +
  theme_few(base_size=10) +
  geom_point(alpha=.3) +
  geom_smooth(span=20, aes(weight=count_frames), alpha=.1) +
  facet_wrap(~category, nrow=2) +
  xlab('Age (in months)') +
  ylab('Proportion detected') +
  theme(aspect.ratio = 1, legend.position='none') +
  scale_size_area(max_size=2)

```


```{r calc-animacy-size, echo=FALSE}
# Get long form data to look at conjunctions
d_by_image <-  d %>%
  distinct(public_url, age_days, age_day_bin, child_id, category, conf) %>%
  spread(key = category, value=conf)

d_by_image <- d_by_image %>%
  mutate(storytime = (Book>0) & (Person>0)) %>%
  mutate(playtime = is.na(Book) & (Person>0) & (Toy>0 | `Vehicle-toy`>0 | `Animal-toy`>0)) %>%
  mutate(mealtime = (Food>0) & (`Utensil-Dish`>0)) %>%
  mutate(kid_things = (`Animal-toy`)>0  | (`Vehicle-toy`)>0 | (`Toy`)>0  | (`Book`)>0 ) %>%
  mutate(animal_all = (`Animal-real`>0) | (`Animal-toy`>0)) %>%
  mutate(big_obj = (BigObj>0) | (Furniture>0) | (`Vehicle-real`>0)) %>%
  mutate(small_obj = (SmallObj>0) | (Toy>0) | (`Utensil-Dish`>0) | (Food>0) | (`Vehicle-toy`)>0  | (`Animal-toy`>0) | (Book>0))

animacy_size <- d_by_image %>%
  group_by(age_day_bin) %>%
  dplyr::summarize(count_anim = sum(animal_all, na.rm=TRUE), count_big = sum(big_obj, na.rm=TRUE), count_small = sum(small_obj, na.rm=TRUE), count_frames = n(), count_people = sum(Person>0, na.rm=TRUE)) %>%
  mutate(Animals = count_anim / count_frames, BigObj = count_big / count_frames, People = count_people / count_frames, SmallObj = count_small / count_frames) %>%
  gather(key = anim_size, value = freq, BigObj, SmallObj, Animals) %>%
  select(-count_anim, -count_big, -count_small) %>%
  mutate(data_type = 'All frames')
```

```{r calc-animacy-size-hands}
d_by_image_hands <-  d_hands %>%
  distinct(public_url, age_days, age_day_bin, child_id, category, conf) %>%
  spread(key = category, value=conf)

d_by_image_hands <- d_by_image_hands %>%
  mutate(storytime = (Book>0) & (Person>0)) %>%
  mutate(playtime = is.na(Book) & (Person>0) & (Toy>0 | `Vehicle-toy`>0 | `Animal-toy`>0)) %>%
  mutate(mealtime = (Food>0) & (`Utensil-Dish`>0)) %>%
  mutate(kid_things = (`Animal-toy`)>0  | (`Vehicle-toy`)>0 | (`Toy`)>0  | (`Book`)>0 ) %>%
  mutate(animal_all = (`Animal-real`>0) | (`Animal-toy`>0)) %>%
  mutate(big_obj = (BigObj>0) | (Furniture>0) | (`Vehicle-real`>0)) %>%
  mutate(small_obj = (SmallObj>0) | (Toy>0) | (`Utensil-Dish`>0) | (Food>0) | (`Vehicle-toy`)>0  | (`Animal-toy`>0) | (Book>0))

animacy_size_hands <- d_by_image_hands %>%
  group_by(age_day_bin) %>%
  dplyr::summarize(count_anim = sum(animal_all, na.rm=TRUE), count_big = sum(big_obj, na.rm=TRUE), count_small = sum(small_obj, na.rm=TRUE), count_frames = n(), count_people = sum(Person>0, na.rm=TRUE)) %>%
  mutate(Animals = count_anim / count_frames, BigObj = count_big / count_frames, People = count_people / count_frames, SmallObj = count_small / count_frames) %>%
  gather(key = anim_size, value = freq, BigObj, SmallObj, Animals) %>%
  select(-count_anim, -count_big, -count_small) %>%
  mutate(data_type = 'Child hands in view')

animacy_size <- animacy_size %>%
  full_join(animacy_size_hands)
```


# Method
## Dataset
The dataset is described in detail in @SAYcam. Children wore Veho Muvi miniature cameras mounted on a custom camping headlamp harness ("headcams") at least twice weekly, for approximately one hour per recording session. One weekly session was on the same day each week at a roughly constant time of day, while the other(s) were chosen arbitrarily at the participating family’s discretion.  Videos captured by the headcam were 640x480 pixels, and a fisheye lens was attached to the camera to increase the field of view to approximately 109 degrees horizontal x 70 degrees vertical. We randomly sampled 24,000 frames from videos of two of the children in the dataset (S, A) over the entire age range (6-32 months of age) At the time of the recording, both children were in single-child households.

## Annotation procedures
### Categories in the infant view
Annotations of the categories in the dataset were obtained using AWS Sagemaker; adult participants were recruited via Amazon Mechanical Turk. Participants viewed one image at a time and selected whether the following categories were present in the shown image: *Animal (real), Animal (toy/drawing), Vehicle (real), Vehicle (toy/drawing), Plant, Clothing, Person, Furniture, Food, Utensil/Dish, Other Small Object, Other Big Object, Book, None of the above*, or *Nothing visible*.  We included *Other Small Object* and *Other Big Object* as categories that participants could use to indicate the presence of objects that fell outside of these categories but were still salient; additional instructions were provided to specify that *Other Big Object* refers to  objects bigger than a chair, and that *Other Small Object* refers to objects small enough to be held with one or two hands [@konkle2013tripartite]. We chose this coding scheme after iterative piloting options and  examining frames ourselves. Two participants annotated each image, and were required to select at least one category before proceeding. Each category annotation in each image was assigned a confidence score (possible range: 0-1, range in dataset: 0.5-1) and individual annotations that had confidence scores below the 25th percentile were excluded from analyses (although all conclusions hold with and without these low-confidence annotations).

```{r disagreement-check}
total_images = 160
num_ambiguous = total_images - (sanity_check$count[sanity_check$reason=="absent"] + sanity_check$count[sanity_check$reason=="present"])
ambiguous = num_ambiguous/total_images
false_alarms = sanity_check$count[sanity_check$reason=="absent"] / total_images
missed = sanity_check$count[sanity_check$reason=="present"] / total_images
```


```{r anim_size, echo=F, include = T, fig.env = "figure*", fig.pos = "h", fig.align='center', fig.width=7, fig.height=3, set.cap.width=T, num.cols.cap=2, fig.align = "center", fig.cap = "Frequency of animals (including toys) relative to big and small inanimate objects detected in the dataset, both when analyzing all frames that were annotated (left) and the subset of frames where a child's hand was visible in the frame (right)."}
# Figure  3
## Animacy/size frequency across age
ggplot(data=animacy_size, aes(x=age_day_bin, y=freq, size=count_frames, col=anim_size, group=anim_size)) +
  geom_point(alpha=.5, aes(group=anim_size)) +
  theme_few(base_size=12) +
  geom_smooth(span=10) +
  xlab('Age (in months)') +
  ylab('Proportion detected') +
  theme(legend.position='right') +
  guides(size = FALSE) +
  facet_wrap(~data_type, nrow=1) +
  scale_color_manual(name="", values = c("purple", "blue", "orange"), labels = c('Animals','Big Objects','Small Objects')) +
  scale_size_area(name = "# of frames")

# maybe remove "Nothing" and "Other"?

```

We assessed the reliability of these annotations by comparing them to annotations made on the same task for a random subsample of 1200 frames on AWS Sagemaker, again using two participants per image ($N$=950 frames after excluding low-confidence annotations). We found agreement was moderate (average Cohen's Kappa = `r round(mean(all_kappas$kappa),2)`, but varied substantially between different categories  (range = `r round(range(all_kappas$kappa),2)`), as Cohen's Kappa is known to be  harsh for sparse annotations. On average, there was disagreement rate of `r round(mean(all_kappas$disagree*100),2)`% across categories between these two samples. Annotators disagreed most on whether *Clothing* was present in an image and whether *Other Big Object* was present (i.e. a big object that was not *Furniture* or a *Vehicle*).  To assess the nature of these disagreements, we manually examined a random sample of 160 images with disagreements with 10 images from each category.  There were relatively equal proportions of images where annotations failed to identify a clear example of a category (M=`r round(missed,2)*100`%) or where annotations select an erroneous category label (M=`r round(false_alarms,2)*100`%). However, we found that most (M=`r round(ambiguous*100,2)`%) of the disagreements resulted from ambiguous exemplars, for example where the category was present but very distant, occluded, or blurry. Annotators also showed some disagreement about whether glossy photos of different categories in books should be counted as "real" or "toy/drawing," and whether partial views of people (i.e. child's own hands) should count as a *Person*.  Going forward, we analyze the larger set of annotations with the caveat that there is inevitably some ambiguity in what counts as an exemplar of these categories (and that these data include both misses and false alarms). All annotations are openly available at the repository associated with this project (https://osf.io/ft4ka/).

```{r basic-level-frames}
frame_basics <- d %>%
  filter(!is.na(basic_level)) %>%
  distinct(public_url, basic_level, age_day_bin, child_id)

frames_with_objs <- frame_basics %>%
  filter(!basic_level %in% c('no child hands','allocentric','no object','unknown object')) 

```
### Objects children interacted with
We also annotated the objects that children were interacting with in a subset of these frames.
To do so, we first selected the frames in which participants (recruited via Amazon Mechanical Turk) indicated that a child's hand or hands were visible in the image [see @long2020] and one author annotated `r  length(frame_basics$public_url)` of these frames, spanning `r min(frame_basics$age_day_bin)` to `r max(frame_basics$age_day_bin)` months of age with roughly equal proportions from the two children. The annotator noted what object the child was touching or pointing to within frames containing children’s hands, using basic-level object categories such as "block" and "cracker." 
If a child was holding a book and pointing to a depicted object in the book, the depicted object was noted as the category they were interacting with; otherwise, it was  noted as *Book*.
Food that was unidentifiable as a specific item (e.g., as crackers) was marked as "food," and baby toys that were unidentifiable as specific toys were marked "toy."
When children were interacting with drawing or toy versions of different categories (e.g., a toy car), these annotations were marked with a ‘-drawing’ and ‘-toy’ modifier and counted as separate entries. 
If a view was allocentric, there were no child hands in view, or there were no objects that were visible, these frames were excluded from analysis; this left `r length(frames_with_objs$public_url)` frames with annotations.


# Results
## Which categories are prevalent in the child's view?
First, we examined the overall prevalence of each category in the infant view. 
Somewhat surprisingly, we found that the prevalence of most of these categories were relatively stable both across the two children in the dataset as well as over developmental time (see Figure \ref{fig:freq_by_category}). 
This stands in contrast to prior work on the prevalence of faces/hands in the infant view [@fausey2016; @long2020], suggesting that these broader characteristics of children's visual experience may be more consistent.

We next examined the details of these environments.
We found that people were by far the most prevalent of these categories: over 20%\ of the annotated frames contained people, far more than any other category (including all kinds of toys combined).
In contrast, there were relatively few instances of animals in the infant view -- either as toys or their real-life counterparts.
Less than 5%\ of the frames contained any kind of depicted or real animal, and those frames contained depicted vs. real animals in equal proportion.
Manual inspection of these frames containing animals revealed that the "real" animals had relatively little variety -- they were overwhelmingly frames containing images of household pets (i.e., cats, dogs, and chickens, in the case of A), whereas the animals that were "toys/drawings" depicted a much larger variety of animals, as one might expect.
Overall, these results suggest that -- at least for these children -- people are much more frequent than depictions or real-life versions of animals, indicating that toys and drawings may provide frequent input to their representations of these categories -- despite the fact that animal names are often among children's first words [@frank2021] and often referenced in storybooks.

Far more prevalent than animals, instead, were objects. 
Views of furniture were the next most common category after people. However, in older age ranges, "big objects" -- including *Furniture*, *Vehicles*, and *Other Big Objects* -- tended to be less frequently in the view of infants than "small" objects -- including *Toys* (of all kinds), *Food*, *Utensils*, *Books*, and *Other Small Objects* (see Figure \ref{fig:anim_size}). 
This effect was much exaggerated when we conducted this analysis on a subset of the frames where children's hands were also in view as a proxy for times when children were interacting with objects. 
In these frames, small objects tended to be much more prevalent in the frames that we annotated. 
These data are consistent with the idea that as children grow and become more adept at handling objects on their own, small objects may tend to be more often in view.

```{r coocc-data, echo=FALSE}
## Co-occurrence statistics

## First get some sort of table (img x category) that makes sense
temp <- d %>%
  group_by(img, category) %>%
  tally() %>%
  pivot_wider(names_from = category, values_from = n, values_fill=0) %>%
  ungroup() %>%
  select(-img)


get_img_category_coocs <- function(temp) {
  col_names = colnames(temp)
  cat_coocs <- matrix(0, nrow=length(col_names),
                         ncol=length(col_names))

  for (c1 in 1:length(col_names)) {
    for (c2 in 1:length(col_names)) {
      if(c1!=c2) cat_coocs[c1,c2] = sum(temp[,c1]==1 & temp[,c2]==1)
    }
  }

  rownames(cat_coocs) = col_names
  colnames(cat_coocs) = col_names
  return(cat_coocs)
}

cat_coocs <- get_img_category_coocs(temp)
# 16 x 16

do_permutation_analysis <- function(temp, Nsim=100) {
  col_names = colnames(temp)
  perm_coocs <- matrix(0, nrow=length(col_names),
                         ncol=length(col_names))
  diag(perm_coocs) = NA
  perm_vec = c() # flat vector of all the values 
  for(i in 1:Nsim) {
    # permute rows
    permed <- t(apply(temp, 1, sample))
    colnames(permed) = colnames(temp)
    new_perm_mat = get_img_category_coocs(permed)
    diag(new_perm_mat) = NA
    perm_coocs = perm_coocs + new_perm_mat
    perm_vec = c(perm_vec, na.omit(as.vector(new_perm_mat)))
    # sum(rowSums(permed)!=rowSums(temp)) # test: yup, same totals per row
  }
  # do we actually want to save the entire flattened matrix of values, and look at the distribution?
  rownames(perm_coocs) = col_names
  colnames(perm_coocs) = col_names
  perm_coocs = perm_coocs / 100 
  return(list(mat=perm_coocs, vec=perm_vec))
}

### Wanted this to work in tidyverse.....sorry
# GK: no worries--it's complicated! I made a classic co-occurrence matrix
# (same as yours but in wide format) to make a heatmap with dendrograms as an alternative

get_img_category_coocs_long <- function(temp) {
  col_names = colnames(temp)
  for (c1 in 1:length(col_names)) {
    for (c2 in 1:length(col_names)) {
      both_present= sum(temp[c1]==1 & temp[c2]==1)
      if (c1==c2){
        both_present=NA
      }
      this_data = tibble(col_names[c1], col_names[c2], both_present)
      colnames(this_data) = c('Category1','Category2','Count')

    if (c1==1 & c2==1){
      coocc_data = this_data
    } 
    else {
      coocc_data <- coocc_data %>%
      full_join(this_data)
    }
    }
  }
  return(coocc_data)
}

coocc_data <- get_img_category_coocs_long(temp)

## Reorder data for plot
# num_images = length(unique(d$img))
# ordered_coocc  <- coocc_data %>%
#   left_join(freq_by_category, by = c('Category1' = 'category')) %>%
#   ungroup() %>%
#   mutate(Category1 = forcats::fct_reorder(Category1, freq_category, .desc=TRUE)) %>%
#   mutate(Frequency = Count / num_images)
#
# ordered_coocc$Category2 = factor(ordered_coocc$Category2, levels = levels(ordered_coocc$Category1))

```

```{r permutation-analysis, eval=F}
# Mike: what about permutation testing to identify those co-occurrences that are outside of chance levels?
# G: permute individual co-occurrences?
# for each row (frame), permute the categories
set.seed(123)
perm_coocs <- do_permutation_analysis(temp)

#perm_95bounds = quantile(perm_coocs$vec, probs = c(0.05, 0.95))
perm_99bounds = quantile(perm_coocs$vec, probs = c(0.01, 0.99))

hist(perm_coocs$vec, main="Distribution of Permuted Co-occurrences", xlab="Permuted Co-occurrence Count")
abline(v=perm_99bounds, col="red")
```

```{r permutation-analysis-outputs}
# (from above permutation analysis; not re-run every time because it's slow)
perm_99bounds = c(535, 647)

freq_by_category_to_join <- freq_by_category %>%
  rename(Category1 = category, Category1_Count = count)

ordered_prob  <- coocc_data %>%
  left_join(freq_by_category_to_join) %>%
  group_by(Category1, Category2) %>%
  mutate(prob = Count / Category1_Count) %>%
  ungroup() %>%
  mutate(Category1 = forcats::fct_reorder(Category1, freq_category, .desc=TRUE)) %>%
  filter(!prob==1)

ordered_prob$Category2 = factor(ordered_prob$Category2, levels = levels(ordered_prob$Category1))
ordered_prob$Sig = ifelse(ordered_prob$Count < perm_99bounds[1], "-", 
                          ifelse(ordered_prob$Count > perm_99bounds[2], "+", ""))
```

```{r coocc_stats, echo=F, include = T, fig.env = "figure", fig.pos = "h", fig.align='center', fig.width=3.5, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.align = "center", fig.cap = "Co-occurrence between categories detected in the dataset. Each cell represents the probability that the category on the y-axis (e.g., clothing) occurs relative to the occurrence of the category on the x-axis (e.g., person). Lighter values indicate higher probabilities of co-occurrence (max=.8, min=0). Permutation analysis was used to determine which cell values were outside the 99\\% confidence intervals of counts: -: less than 99\\%, +: greater than 99\\%."}
# Plot co-occurrence stats
ggplot(ordered_prob, aes(x=Category1, y=Category2, fill=prob)) +
  geom_tile() +
  geom_text(aes(label=Sig), color="white") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust=.5)) +
  scale_fill_viridis(option='D', begin=.1, end=.9) +
  ylab('Frequency of occurrence') +
  xlab('Relative to') +
  theme(legend.position='none')

# alternative
#require(gplots)
#heatmap.2(x=cat_coocs, scale="row", col="bluered")
```


## Which categories co-occur in children's visual environment?
Next, we examined the degree to which these categories appeared together in different frames.
Figure \ref{fig:coocc_stats} shows the co-occurrence of these categories, and reveals some relatively intuitive patterns that may reflect activity contexts.
For example, *Dishes* and *Food* co-occurred quite frequently together, as did *People* and *Clothing*, and most *Animals* that were toys or drawings appeared when *Books* were also present.
To determine which cells significantly deviate from chance we used a permutation analysis in which we shuffled the annotated category labels within each frame and examined the distribution of co-occurrences across 100 randomized co-occurrence matrices.
The cells in the plot that occurred fewer times than expected by chance (<99% of permuted cells) are labeled with a '-', while those that occurred more often than expected by chance (>99% of permuted cells) are labeled with a '+'.
These results suggest a strong co-occurrence structure rather than random occurrence, plausibly driven by activity contexts -- such as playtime, mealtime, or storytime [@bruner1985role].


## What objects do children tend to interact with?
```{r basic-level-data}
d_basic <- d %>%
  filter(!is.na(basic_level)) %>%
  mutate(age_group = cut(age_day_bin, c(5,12,18,34), labels = c('6-12m','12-18m','18-32m'))) %>%
  filter(!basic_level %in% c('no child hands','allocentric','no object','unknown object','no objects','no child')) %>%
  distinct(public_url, basic_level, child_id, age_days, age_day_bin, age_group) %>%
  ungroup() 

# to check relative to age distribution in larger set
d_age_check <- d %>%
  distinct(public_url, basic_level, child_id, age_days) 
```

```{r basic-level-filtered}
# colllapse across 
d_basic <- d_basic %>%
  mutate(basic_level_all_formats = str_replace(basic_level, '-drawing','')) %>%
  mutate(basic_level_all_formats = str_replace(basic_level_all_formats, '-toy','')) 

d_basic_limited <- d_basic %>%
  filter(!basic_level %in% c('food','toy','book','person')) 
```

```{r basic-level-for-plots}
count_by_category <- d_basic %>%
  mutate(total_images = n()) %>%
  group_by(basic_level)  %>%
  summarize(count = length(public_url), total_images = total_images[1]) %>%
  mutate(Freq = count/total_images) %>%
  ungroup() %>%
  mutate(basic_level = fct_reorder(basic_level, Freq, .desc=TRUE))
```

```{r basic-level-stats}
length_annotations = length(d_basic$basic_level)
basic_level_categories = (unique(d_basic$basic_level))
basic_level_all_formats = (unique(d_basic$basic_level_all_formats))

obj_freq = sort(table(d_basic$basic_level), desc=T)
pl_fit <- power.law.fit(obj_freq)

obj_freq_all_formats = sort(table(d_basic$basic_level_all_formats), desc=T)
pl_fit_formats <- power.law.fit(obj_freq_all_formats)

obj_freq_limited = sort(table(d_basic_limited$basic_level), desc=T)
pl_fit_limited <- power.law.fit(obj_freq_all_formats)

```

```{r basic_levelexamples, include = T, fig.env = "figure", fig.pos = "h", fig.align='center', fig.width=4, out.width="80%", set.cap.width=T, num.cols.cap=1, fig.align = "center", fig.cap = "Example frames where we annotated the basic-level categories of objects children interacted with; many of these frames contained books and generic toys."}
basic_examples <- png::readPNG("figs/basic_level_examples.png")
grid::grid.raster(basic_examples)
```

While many different categories may be in the child's view, not all of these objects may be experienced in the same way.
In particular, it may be that children are more likely to form robust representations of objects that they physically interact with more often, and by extension they may also learn the labels of these objects earlier.
In this analysis, we sought to analyze the basic-level identities of the objects that children tended to be interacting with in their home environments, and the distributions of those identities.
While some work has found that the objects in view during mealtime tend to have a Zipfian distribution [@clerkin2017], it is not yet known whether this finding will extend to objects that do not appear during mealtime and that children interact with during a wide range of activities.
For example, there may be far fewer objects that are only interacted with a limited number of times vs. seen a limited number of times.

In the `r length_annotations` frames with objects that children were interacting with, we found `r length(basic_level_all_formats)` unique categories when collapsing across formats (i.e. drawings, toys, real-life), and `r length(basic_level_categories)` unique categories when exemplars were considered separately across formats.
When we examined which categories were most frequent, we found that books were overwhelmingly the most present object in the views of these two children, comprising over 20\% of the objects that these children were seen to be interacting with.
Generic baby toys (that were unidentifiable to the authors as specific toys) were the next most prevalent object category, and children were often seen to be touching or holding on to their caregivers (see top 20 most frequent categories in Figure \ref{fig:freq_interact} and example frames in Figure \ref{fig:basic_levelexamples}).
Further, these three categories -- *book*, *toy*, and *person* -- were consistently the top three most frequent when we examined data separately for each child and by age groups (6-12 months, 12-18 months, 18-24 months).

Importantly, we found that the distribution of the objects children were interacting with roughly followed a power law distribution, when we included separate categories for different formats  ($\alpha =$ `r round(pl_fit$alpha,2)`), when we collapsed across them ($\alpha =$ `r round(pl_fit_formats$alpha,2)`), or when we excluded *book*, *toy*, and *person* ($\alpha =$ `r round(pl_fit_limited$alpha,2)`). Thus, overall these results confirm that the distribution of the objects that children interact with is highly skewed, generalizing the findings of Clerkin et al., 2017.


```{r include=FALSE}
count_by_category_A <- d_basic %>%
  filter(!basic_level %in% c('no child hands','allocentric','no object','unknown object')) %>%
  filter(child_id == 'A') %>%
  distinct(public_url, basic_level) %>%
  ungroup() %>%
  mutate(total_images = n()) %>%
  group_by(basic_level)  %>%
  summarize(count = length(public_url), total_images = total_images[1]) %>%
  mutate(Freq = count/total_images) %>%
  ungroup() %>%
  mutate(basic_level = fct_reorder(basic_level, Freq, .desc=TRUE)) %>%
  arrange(desc(Freq))

count_by_category_S <- d_basic %>%
  filter(!basic_level %in% c('no child hands','allocentric','no object','unknown object')) %>%
  filter(child_id == 'S') %>%
  distinct(public_url, basic_level) %>%
  ungroup() %>%
  mutate(total_images = n()) %>%
  group_by(basic_level)  %>%
  summarize(count = length(public_url), total_images = total_images[1]) %>%
  mutate(Freq = count/total_images) %>%
  ungroup() %>%
  mutate(basic_level = fct_reorder(basic_level, Freq, .desc=TRUE)) %>%
  arrange(desc(Freq))

```

```{r freq_interact, echo=F, include = T, fig.env = "figure", fig.pos = "h", fig.align='center', fig.width=3, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.align = "center", fig.cap = "Top 20 most frequent categories that children's hands were interacting with in these egocentric videos."}
ggplot(data=count_by_category %>% top_n(20), aes(x=basic_level, y=Freq)) +
  geom_bar(stat = "identity") +
  theme_few(base_size=10) +
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust=1)) +
  xlab('')  +
  ylab('Frequency of occurrence') +
  ylim(0,.3)
```

```{r include=FALSE}
# S = ggplot(data=count_by_category_S %>% top_n(20), aes(x=basic_level, y=Freq)) +
#   geom_bar(stat = "identity") +
#   theme_few(base_size=10) +
#   theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust=1)) +
#   xlab('')  +
#   ylab('Frequency of occurrence') +
#   ylim(0,.3)
# 
# A = ggplot(data=count_by_category_A %>% top_n(20), aes(x=basic_level, y=Freq)) +
#   geom_bar(stat = "identity") +
#   theme_few(base_size=10) +
#   theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust=1)) +
#   xlab('')  +
#   ylab('Frequency of occurrence') +
#   ylim(0,.3)


# ggarrange(S,A, nrow=2)
```

```{r include=FALSE}

count_by_category_by_age <- d_basic %>%
  distinct(public_url, age_group, basic_level) %>%
  group_by(age_group) %>%
  mutate(total_images = n()) %>%
  group_by(age_group, basic_level)  %>%
  summarize(count = length(public_url), total_images = total_images[1]) %>%
  mutate(Freq = count/total_images)

count_by_category_youngest <- count_by_category_by_age %>%
  filter(age_group == '6-12m') %>%
  mutate(basic_level = fct_reorder(basic_level, Freq, .desc=TRUE)) %>%
  arrange(desc(Freq)) 

count_by_category_mid <- count_by_category_by_age %>%
  filter(age_group == '12-18m') %>%
  mutate(basic_level = fct_reorder(basic_level, Freq, .desc=TRUE))   %>%
  arrange(desc(Freq))

count_by_category_oldest <- count_by_category_by_age %>%
  filter(age_group == '18-32m') %>%
  mutate(basic_level = fct_reorder(basic_level, Freq, .desc=TRUE))   %>%
  arrange(desc(Freq)) 
```


# General Discussion
What determines the categories that infants tend to see and interact with across early development? To examine the categories in the infant view, we analyzed a sample of random frames taken from a longitudinal dataset of two children [@SAYcam].
Overall, we found relative stability in these two children's visual environment over development, in contrast to prior work on the prevalence of social signals over this same developmental time period [@fausey2016; @long2020].
The relative proportions of categories of objects (i.e., *furniture*, *toys*, *animals*, *people*) were consistent among the two individuals here, and across developmental time. 
People were most frequent, and a non-trivial proportion of frames didn't contain any discernible objects at all.
However, these categories co-occured together in reliable patterns, revealing stereotypical combinations (i.e. *utensil/dish* and *food*, *person* and *clothing*) and suggesting that activity contexts, such as mealtime or storytime [@bruner1985role] may structure the broad characteristics of young children's visual environment.

Yet while people were incredibly frequent in the child's view, animals -- either as toys or their real-life versions -- were relatively infrequent and occurred in equal proportions. 
This finding stands in contrast to a long literature documenting that even newborns prefer to attend to animate agents [@farroni2005newborns], that visual cortex dedicates a remarkable amount of space to processing animals [@konkle2013tripartite], and that animal names tend to be among children's first-learned words [@frank2021].  
Therefore,  children's heightened attention to animals [@farroni2005newborns] likely interacts with frequency of occurrence in the visual field to drive early category learning.

Instead, we found that these children's viewpoints were likely to be dominated by small objects -- such as food, books, or toys -- especially when their own hands were present in the frame, suggesting that the statistics of children's visual environment shift substantially when they are acting on the world themselves. 
We also found that the distribution of these objects seem to follow a Zipfian distribution, as does word usage in natural language.
While mealtime has previously been used to characterize the objects in the infant view [@clerkin2017] and frames with food or utensil and dishes accounted for less than 5%\ of views in the SAYcam dataset, we were unsure whether this also would be the case.
However, the present analysis suggests that infants' interactions with different object categories may be Zipf-distributed, with most categories seen quite rarely, and a few categories dominating their experience.

If this result is generalizable, and children’s visual experience is as skewed as their language experience -- with the majority of referents appearing rarely and a minority of referents appearing frequently -- this distributional regularity provides strong constraints on the category and word learning mechanisms that can succeed in this environment [@lavi2021visual; @hidaka2017quantifying].  
Indeed, the skewed distributions found here and in @clerkin2017 are dramatically different than the uniform distributions of categories fed to modern models of visual category learning -- which nonetheless appear to mimic many aspects of the visual system [@jacob2021qualitative].  
Future work that equates the visual learning environments experienced by children and computational models of category learning may elucidate the set of learning mechanisms needed to form robust representations from realistic visual inputs.

This work thus takes a first step in characterizing the categories in the visual environment over early development, calling for future work to understand the generalizability of these findings and their implications for models of early category.
While we found consistent results across both age and the two children analyzed here, both children are from similar households, socioeconomic groups, and cultural contexts.
Furthermore, these recordings were only taken when children were home with their parents (vs. at daycare), and these parents may have changed how they interacted with their children during these recording sessions.
Variation across different households and communities will certainly change the objects that are frequently in the infant view: for example, picture books and toys are unlikely to be present in the views of many children in subsistence farming communities [@casillas2020early].
Nonetheless, we predict that the distribution of objects that children interact will continue to follow a Zipfian distribution -- regardless of when or where recordings are taken or which specific objects are most frequently experienced.

Of course, these results do not preclude the possibility that there are finer grained changes in how children experience object categories that change the information that they encode. 
For example, the current analysis supports the intuition that toys and books are prevalent in the views of some infants, it does not document how children are interacting with these toys or what categories in the books their caregivers may be pointing out. 
More detailed coding schemes of the categories in the infant view could also yield divergent results.
We propose that moving towards finer-grained analysis of the activities in naturalistic videos may uncover more subtle developmental trends.  

Overall, this work highlights the need for systematic investigations of how the frequency of the categories in the child's view interacts with different attentional biases, learning mechanisms, and social cues to produce robust representations that support early category and language learning.
An understanding of what is -- and what is not -- learnable solely from frequent exposures will provide constraints on our accounts of the learning mechanisms that allow children to learn so much so quickly.

\vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering All data and code for these analyses are available at\ \url{https://osf.io/ft4ka/}}} \vspace{1em}

# Acknowledgements
We gratefully acknowledge the creators of the SAYCam dataset who made this work possible. This work was funded by a HAI Cloud Credit Grant to BL and MCF, a Jacobs Foundation Fellowship to MCF, a John Mereck Scholars award to MCF, and NSF #1714726 to BLL. 


# References
```{r echo=F}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in}
\setlength{\leftskip}{0.125in}
\noindent
